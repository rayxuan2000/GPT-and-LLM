{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayxuan2000/GPT-and-LLM/blob/main/baby_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Build a GPT from scratch, teach the baby-GPT a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgGbyO-4qKk8"
      },
      "source": [
        "### prepare our simple dataset, for instance, one sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "text = \"Your name is GPT-3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "0856bab3-c50d-4381-d318-6f84d7f3d1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  18\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "d7a286fc-1a63-44b1-dc89-0ec1d22670d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is GPT-3\n"
          ]
        }
      ],
      "source": [
        "# check our text\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e-Rbyr8sfM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010258bd-3d3a-4aa1-9e4a-c5e5cf1b4964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters from the sentence:  -3GPTYaeimnorsu\n",
            "vocab_size from the sentence:  16\n"
          ]
        }
      ],
      "source": [
        "# Identifies all unique characters present in the given text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Characters from the sentence:\", \"\".join(chars))\n",
        "print(\"vocab_size from the sentence: \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary dictionary mapping each character to a unique index\n",
        "vocab_table = {}\n",
        "for i, char in enumerate(chars):\n",
        "    vocab_table[char] = i\n",
        "\n",
        "# Collect keys (characters) and values (indexes) from the vocabulary table\n",
        "characters = list(vocab_table.keys())\n",
        "indexes = list(vocab_table.values())\n",
        "\n",
        "# Print the characters and their corresponding indexes in tab-separated format\n",
        "print(\"Character\\t\", \"\\t\".join(characters))  # Print all characters separated by tabs\n",
        "print(\"Index\\t\\t\", \"\\t\".join(map(str, indexes)))  # Print all indexes separated by tabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDKjuCayqd9_",
        "outputId": "63b29694-0310-47f3-c3c0-6fac916f8f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character\t  \t-\t3\tG\tP\tT\tY\ta\te\ti\tm\tn\to\tr\ts\tu\n",
            "Index\t\t 0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw1LKNCgwjj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75439258-f486-4aaf-da08-8ab0b8e2d613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function <lambda> at 0x7f4b619de7a0> <class 'function'>\n",
            "Your name is GPT-3\n",
            "[6, 12, 15, 13, 0, 11, 7, 10, 8, 0, 9, 14, 0, 3, 4, 5, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "print(encode, type(encode))\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "print(decode(encode(text)))\n",
        "print(encode(f\"{text}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJb0OXPwzvqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c65194-70e2-4349-e80d-7ee124135160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([18]) torch.int64\n",
            "tensor([ 6, 12, 15, 13,  0, 11,  7, 10,  8,  0,  9, 14,  0,  3,  4,  5,  1,  2])\n",
            "{'Your name is GPT-3'}\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "real_text = lambda tensor: { decode(tensor.tolist()) }\n",
        "real_char = lambda tensor: { decode([tensor.item()]) }\n",
        "\n",
        "# let's print the data\n",
        "print(data.shape, data.dtype)\n",
        "print(data)\n",
        "print(real_text(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gve7ixvqKlA"
      },
      "source": [
        "### Use the sentence for training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are concerned about text generation here, which means given some texts what would be the next token (character)?"
      ],
      "metadata": {
        "id": "zNArMdtN_chQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "train_data = data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### example"
      ],
      "metadata": {
        "id": "qer0aST6_r7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD5Bj8Y6IAD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4ec286-f8ed-45f9-a862-857d6334c480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 6, 12, 15])\n",
            "{'You'}\n",
            "tensor([12, 15, 13])\n",
            "{'our'}\n"
          ]
        }
      ],
      "source": [
        "block_size = 3 # how many tokens are there in one batch\n",
        "block_data = train_data[:block_size]\n",
        "target_data =  train_data[1:block_size+1]\n",
        "print(block_data)\n",
        "print(real_text(block_data))\n",
        "print(target_data)\n",
        "print(real_text(target_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HXDe8vGJCEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6526f0-e396-4c7d-f54d-6412a2796e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([6])--'{'Y'}', the target is: [12]--'{'o'}'\n",
            "when input is tensor([ 6, 12])--'{'Yo'}', the target is: [15]--'{'u'}'\n",
            "when input is tensor([ 6, 12, 15])--'{'You'}', the target is: [13]--'{'r'}'\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    real_context = real_text(context)\n",
        "    real_target = real_char(target)\n",
        "    print(f\"when input is {context}--'{real_context}', the target is: [{target}]--'{real_target}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### implementation"
      ],
      "metadata": {
        "id": "KypQz1R7AKOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7CuCwmvu5sX",
        "outputId": "56ada8a1-ca72-4032-f975-8dea8c64db5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 6, 12, 15, 13,  0, 11,  7, 10,  8,  0,  9, 14,  0,  3,  4,  5,  1,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3k1Czf7LuA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24db095f-a920-4707-e0e6-5a904d11517b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 5])\n",
            "tensor([[ 0,  9, 14,  0,  3],\n",
            "        [ 0,  9, 14,  0,  3],\n",
            "        [10,  8,  0,  9, 14],\n",
            "        [14,  0,  3,  4,  5]])\n",
            "{' is G'}\n",
            "{' is G'}\n",
            "{'me is'}\n",
            "{'s GPT'}\n",
            "targets:\n",
            "torch.Size([4, 5])\n",
            "tensor([[ 9, 14,  0,  3,  4],\n",
            "        [ 9, 14,  0,  3,  4],\n",
            "        [ 8,  0,  9, 14,  0],\n",
            "        [ 0,  3,  4,  5,  1]])\n",
            "{'is GP'}\n",
            "{'is GP'}\n",
            "{'e is '}\n",
            "{' GPT-'}\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "# block_size = len(data) - 1 # what is the maximum context length for predictions?\n",
        "block_size = 5 # what is the maximum context length for predictions?\n",
        "\n",
        "\n",
        "def get_batch():\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    data = train_data\n",
        "\n",
        "    # generates batch_size number of random starting indices\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch()\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "for x in xb:\n",
        "    print(real_text(x))\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "for y in yb:\n",
        "    print(real_text(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLJpAYPhqKlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c75f9f0-16cf-4e80-e371-824c1ce1fd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0])--' ', the target is: [9]--'i'\n",
            "when input is tensor([0, 9])--' i', the target is: [14]--'s'\n",
            "when input is tensor([ 0,  9, 14])--' is', the target is: [0]--' '\n",
            "when input is tensor([ 0,  9, 14,  0])--' is ', the target is: [3]--'G'\n",
            "when input is tensor([ 0,  9, 14,  0,  3])--' is G', the target is: [4]--'P'\n",
            "when input is tensor([0])--' ', the target is: [9]--'i'\n",
            "when input is tensor([0, 9])--' i', the target is: [14]--'s'\n",
            "when input is tensor([ 0,  9, 14])--' is', the target is: [0]--' '\n",
            "when input is tensor([ 0,  9, 14,  0])--' is ', the target is: [3]--'G'\n",
            "when input is tensor([ 0,  9, 14,  0,  3])--' is G', the target is: [4]--'P'\n",
            "when input is tensor([10])--'m', the target is: [8]--'e'\n",
            "when input is tensor([10,  8])--'me', the target is: [0]--' '\n",
            "when input is tensor([10,  8,  0])--'me ', the target is: [9]--'i'\n",
            "when input is tensor([10,  8,  0,  9])--'me i', the target is: [14]--'s'\n",
            "when input is tensor([10,  8,  0,  9, 14])--'me is', the target is: [0]--' '\n",
            "when input is tensor([14])--'s', the target is: [0]--' '\n",
            "when input is tensor([14,  0])--'s ', the target is: [3]--'G'\n",
            "when input is tensor([14,  0,  3])--'s G', the target is: [4]--'P'\n",
            "when input is tensor([14,  0,  3,  4])--'s GP', the target is: [5]--'T'\n",
            "when input is tensor([14,  0,  3,  4,  5])--'s GPT', the target is: [1]--'-'\n"
          ]
        }
      ],
      "source": [
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        real_context = decode(context.tolist())\n",
        "        real_target = decode([target.item()])\n",
        "        print(f\"when input is {context}--'{real_context}', the target is: [{target}]--'{real_target}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpyyAeIzQjlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fe7a0a-dc11-4038-a20b-4805e4339931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  9, 14,  0,  3],\n",
            "        [ 0,  9, 14,  0,  3],\n",
            "        [10,  8,  0,  9, 14],\n",
            "        [14,  0,  3,  4,  5]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuHmen-uqKlB"
      },
      "source": [
        "### Build our simple NLP model with our tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nql_1ER53oCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d971be0-7aa8-495f-9594-72df5437efb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "torch.Size([20, 16])\n",
            "tensor(3.4646, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class SimpleNLPModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        # data flow: embedding -> linear layer -> logits\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            # each C dimension tensor is a representation of a token\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens): # used for generating new text\n",
        "\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)    1: 0.1 2: 0.2 3: 0.7 -> 3   max\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            # you -> r   you + r = your\n",
        "            # your -> \" \"\n",
        "            # your -> n\n",
        "        return idx\n",
        "\n",
        "m = SimpleNLPModel(vocab_size)\n",
        "print(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlJfIZIOqKlB"
      },
      "source": [
        "### Try to generate something with untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxHgMEOZqKlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb80b6d-8356-4e66-b124-68a29532160a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate something with untrained model:\n",
            " eT TYneP-TanroT uoGYnamGsPes-smYrYGrrrrPomT uPsm-o P-uuT PnissmT aeoPr uuomiaa333ai ia isr eTiT T oi\n"
          ]
        }
      ],
      "source": [
        "print(\"Generate something with untrained model:\")\n",
        "# start with space char!\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is meaningless and super random!"
      ],
      "metadata": {
        "id": "T1CFbLNuG3qY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpZTvC0RqKlC"
      },
      "source": [
        "### let's train our simple nlp model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs4kI8YdEkQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e60bfb-cf54-4e0e-f047-6c69a2976814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.7557: 100%|██████████| 5000/5000 [00:19<00:00, 256.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7556718587875366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "m = m.to(device)\n",
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "# Use tqdm to add a progress bar and display intermediate loss values in the progress bar\n",
        "pbar = tqdm(range(5000))\n",
        "for steps in pbar:\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch()\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    # Evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the progress bar description to show the current loss value\n",
        "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAneSG1CqKlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2145423b-5971-4387-af5c-7253b807be67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate something with trained simple model:\n",
            "-3TGPT-33ami GPTouYr name Yr GPT name nr isrGPme GPT-ais name name nmeT-ur name is GPeeuouu-uour is Y\n"
          ]
        }
      ],
      "source": [
        "print(\"Generate something with trained simple model:\")\n",
        "print(decode(m.generate(idx = torch.ones((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))\n",
        "# print(real_char(torch.ones((1, 1))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems it has improved a little but there is still a long way to go!"
      ],
      "metadata": {
        "id": "Rgbb64-xHbRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcVIDWAZEtjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc09814d-b9a2-42d8-cffd-7f08ac5c8650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yr name Gre GPTYGYr i-ur nme GPPGYr GPT GPT-33Pr3Genam3 isYoui-amis our is namens is nam-3ame is GT-s\n"
          ]
        }
      ],
      "source": [
        "start_id = torch.tensor([encode('Y')], dtype=torch.long)\n",
        "start_id = start_id.to(device)\n",
        "# start with 'Y'!\n",
        "print(decode(m.generate(idx = start_id, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Illustration of all shapes"
      ],
      "metadata": {
        "id": "EkoBLHWIImL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85xAcr3xqKlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46052eb-33ed-4cd4-fb9c-1fc2cb53a21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_size: 5\n",
            "data[:block_size]: {'Your '}\n",
            "The shape of idx (B,T): torch.Size([1, 5])\n",
            "The shape of targets (B,T): torch.Size([1, 5])\n",
            "The shape of token_embedding_table (C,C): torch.Size([16, 16])\n",
            "The batch size is: 1\n",
            "The length of the sequence is: 5\n",
            "The vocab size is: 16\n",
            "targets: tensor([[12, 15, 13,  0, 11]], device='cuda:0')\n",
            "logits[0]: tensor([[-2.9378, -2.6484, -1.4984, -0.4172, -1.3095, -2.0510, -1.6252, -1.3233,\n",
            "         -1.5096, -1.4677, -1.0579, -0.0120,  1.4445,  1.0971, -1.9537, -3.2321],\n",
            "        [-1.5576, -2.0704, -0.0153, -0.4037, -0.7045, -2.3323, -2.1822, -1.1957,\n",
            "         -1.7236, -3.3376, -0.6676, -1.2417, -2.9353, -2.6657, -1.8229,  1.8449],\n",
            "        [-0.7712, -2.2525, -2.9893, -3.3333, -1.8029, -2.4856, -2.2264, -3.0594,\n",
            "         -2.5246, -2.0379, -3.1098, -3.4164, -1.1366,  0.3925, -2.7221, -1.1737],\n",
            "        [ 2.3739, -2.7966, -2.3969, -2.5359, -1.4751, -1.5105, -0.9007, -1.3898,\n",
            "         -1.6100, -1.2559, -2.3605, -2.8164, -2.6586, -1.7395, -2.6319, -2.4802],\n",
            "        [-2.9593, -3.2027, -3.4838,  0.9451, -2.5273, -3.1100, -2.2082, -3.0723,\n",
            "         -2.7842,  1.1699, -4.4451,  0.9954, -2.9057, -3.3675, -4.0289, -1.6366]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "targets: tensor([12, 15, 13,  0, 11], device='cuda:0')\n",
            "Predicted labels: tensor(15, device='cuda:0')\n",
            "{'Your name is GPT-3'}\n",
            "tensor([[12, 15, 13,  0, 11]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def forward_with_log(self, idx, targets=None):\n",
        "\n",
        "    print(\"The shape of idx (B,T):\",idx.shape)\n",
        "    print(\"The shape of targets (B,T):\",targets.shape)\n",
        "    print(\"The shape of token_embedding_table (C,C):\",self.token_embedding_table.weight.shape)\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "    print(\"The batch size is:\", batch_size)\n",
        "    print(\"The length of the sequence is:\", block_size)\n",
        "    print(\"The vocab size is:\", vocab_size)\n",
        "    print(\"targets:\", targets)\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        print(\"logits[0]:\", logits)\n",
        "        targets = targets.view(B*T)\n",
        "        print(\"targets:\", targets)\n",
        "        loss = F.cross_entropy(logits, targets)   #   [1,2,3] -- [1,2,6]  -- 0 [1,3,9]  --  0 -> 1 (99%)  tar: [1,2,4]  --  [1,2,4]  - loss 0\n",
        "\n",
        "    # get the most likely token\n",
        "    _, predicted_labels = torch.max(logits[1], dim=-1)\n",
        "\n",
        "    print(\"Predicted labels:\", predicted_labels)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "m.forward = forward_with_log.__get__(m)\n",
        "m = m.to(device)\n",
        "data = data.to(device)\n",
        "print(\"block_size:\",block_size)\n",
        "print(\"data[:block_size]:\",real_text(data[:block_size]))\n",
        "m.forward(data[:block_size].unsqueeze(0), data[1:block_size+1].unsqueeze(0))\n",
        "print(real_text(data))\n",
        "print(data[1:block_size+1].unsqueeze(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "### The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tukiH-NbRBhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d3089e-461f-456c-9fd1-86943983a1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei=\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.],\n",
            "        [0., 4.],\n",
            "        [0., 3.],\n",
            "        [8., 4.],\n",
            "        [0., 4.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333],\n",
            "        [3.5000, 5.0000],\n",
            "        [2.8000, 4.6000],\n",
            "        [3.6667, 4.5000],\n",
            "        [3.1429, 4.4286]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "\n",
        "\n",
        "# you are the best\n",
        "# 1    2    3   4\n",
        "torch.manual_seed(42)\n",
        "wei = torch.tril(torch.ones(7, 7)) # lower trianguler matrix\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(7,2)).float()\n",
        "c = wei @ b\n",
        "print('wei=')\n",
        "print(wei)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "\n",
        "     # you are the best\n",
        "\n",
        "# you  11  21   23  23\n",
        "# are\n",
        "# the\n",
        "# best\n",
        "\n",
        "\n",
        "# One important observation here is that in wei matrix, each row is like a batch,\n",
        "# we only care about using preceding tokens and current token to predict current\n",
        "# stuff. So this natural auto-regressive structure can be intuitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs_E24uRE8kr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bd0820-bf9c-4e36-f581-772138e7f215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "# x = torch.ones(B,T,C)\n",
        "x.shape\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDarxEWIRMKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c787a463-6add-4c74-960c-9cd7c7769826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
            "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
            "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16 # hyperparameter\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "print(wei[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"masked:\",wei[0])\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"masked:\",wei[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHEnRYFN9tTm",
        "outputId": "ad3aa0bd-ba6f-4ce7-a628-ca6c0b245c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked: tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "masked: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCOQkwG09vPE",
        "outputId": "ec3816b5-b2be-4d6a-f30a-869b8c8423d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. **This block here is called a \"decoder\" attention block** because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "# example to illustrate scaled attention\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1)  #* head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "ec225c43-cc5e-46d2-decc-c769d14c6a36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "349cdf46-20fb-4b54-cc4c-4a548c62b06f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "eb1da8aa-d2f9-4015-ffa4-468b4d9239ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.4690)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "65b56bc6-5b04-4fae-a213-b3fcab76b7df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpt8569BB9_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b07c70-c8bb-4441-c2da-3ba666508fb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1999, 0.1925, 0.2049, 0.1925, 0.2101])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])/8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bma7-tPqKlF"
      },
      "source": [
        "### Add attention magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDujjE77qKlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b80ce58-5975-4eb1-8a54-441cad9c381c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters from the sentence: \n",
            " ,-.ADFGIOPTWabcdefghiklmnopqrstuvwxyz—\n",
            "vocab_size from the sentence:  40\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "text = \"\"\"GPT, short for Generative Pre-trained Transformer, represents a\n",
        "       groundbreaking advancement in the field of artificial intelligence\n",
        "       and natural language processing. Developed by OpenAI, GPT is designed\n",
        "       to understand, generate, and interpret human language with remarkable\n",
        "       accuracy and fluency. It operates on the principle of machine learning,\n",
        "       where the model is initially pre-trained on a vast corpus of text data.\n",
        "       This pre-training enables GPT to grasp the intricacies of language,\n",
        "       including grammar, context, and even subtleties like humor and sarcasm.\n",
        "       Following the pre-training phase, GPT undergoes fine-tuning, where it is\n",
        "       further trained on a smaller, more specialized dataset to perform\n",
        "       specific tasks like translation, question-answering, and content\n",
        "       creation. What sets GPT apart is its deep learning architecture,\n",
        "       which consists of multiple layers of transformers—hence the name.\n",
        "       These transformers allow the model to process and analyze text in a\n",
        "       highly efficient and nuanced manner, making GPT capable of generating\n",
        "       text that is often indistinguishable from that written by humans.\n",
        "       As technology evolves, GPT continues to push the boundaries of what\n",
        "       artificial intelligence can achieve in understanding and mimicking human language.\"\"\"\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Characters from the sentence:\", \"\".join(chars))\n",
        "print(\"vocab_size from the sentence: \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(20, 30)\n",
        "input = torch.randn(128, 20)\n",
        "output = m(input)\n",
        "print(output.size())"
      ],
      "metadata": {
        "id": "dKRUXycUXGoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9ebe39-90c5-43da-f700-07c2dbbbf334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "train_data = data\n",
        "def get_batch():\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = len(data) - 1 # what is the maximum context length for predictions?\n",
        "# block_size = 192 # what is the maximum context length for predictions?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "n_embd = 64 # embd_dim\n",
        "n_head = 1\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,H)\n",
        "        q = self.query(x) # (B,T,H)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,H)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # projects the concatenated output back to the original embedding size.\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # mimic original paper: The dimensionality of input and output is d_model = 512,\n",
        "        # and the inner-layer has dimensionality d_ff = 2048.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    \"\"\" stuff inside the solid line box in the paper \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "        # Layer normalization ensures that each feature has zero mean and unit\n",
        "        # variance for each individual sample, making it effective for\n",
        "        # stabilizing and accelerating the training of neural networks.\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the order is a little different from the original paper\n",
        "\n",
        "        # don't forget residual path\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BabyGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Lv10ujDNAAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meUMvgr-qKlG"
      },
      "source": [
        "### let's train our babyGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-CBoXSbqKlH"
      },
      "outputs": [],
      "source": [
        "m = BabyGPT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAFhHGtGqKlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da66d9bc-5d9d-44ce-f526-c0aadaa2db56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.0010: 100%|██████████| 1000/1000 [00:18<00:00, 53.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009724152041599154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "m = m.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "# 使用tqdm添加进度条，并在进度条中显示中间loss值\n",
        "pbar = tqdm(range(1000))\n",
        "for steps in pbar:\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch()\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 更新进度条的描述以显示当前的loss值\n",
        "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOIlG6axqKlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d196cdd1-d6ac-43da-e4cf-5b90a7db3bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT, short for Generati\n"
          ]
        }
      ],
      "source": [
        "start_id = torch.tensor([encode('GPT')], dtype=torch.long)\n",
        "start_id = start_id.to(device)\n",
        "print(decode(m.generate(idx = start_id, max_new_tokens=20)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1BZEnMffi-x"
      },
      "source": [
        "### Averaged attention GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjW1E9Ssfi-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e67df3-0c02-43c5-cf99-61ba9d9a79c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters from the sentence:  ,-.ADFGIOPTWabcdefghiklmnopqrstuvwxyz—\n",
            "vocab_size from the sentence:  39\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "text = 'GPT, short for Generative Pre-trained Transformer, represents a groundbreaking advancement in the field of artificial intelligence and natural language processing. Developed by OpenAI, GPT is designed to understand, generate, and interpret human language with remarkable accuracy and fluency. It operates on the principle of machine learning, where the model is initially pre-trained on a vast corpus of text data. This pre-training enables GPT to grasp the intricacies of language, including grammar, context, and even subtleties like humor and sarcasm. Following the pre-training phase, GPT undergoes fine-tuning, where it is further trained on a smaller, more specialized dataset to perform specific tasks like translation, question-answering, and content creation. What sets GPT apart is its deep learning architecture, which consists of multiple layers of transformers—hence the name. These transformers allow the model to process and analyze text in a highly efficient and nuanced manner, making GPT capable of generating text that is often indistinguishable from that written by humans. As technology evolves, GPT continues to push the boundaries of what artificial intelligence can achieve in understanding and mimicking human language.'\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Characters from the sentence:\", \"\".join(chars))\n",
        "print(\"vocab_size from the sentence: \", vocab_size)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "train_data = data\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = len(data) - 1 # what is the maximum context length for predictions?\n",
        "# block_size = 32 # what is the maximum context length for predictions?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "n_embd = 64\n",
        "n_head = 1\n",
        "n_layer = 1\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = torch.tril(torch.ones(B, T, T, device=device)) # (B, T, T)\n",
        "        wei = wei / torch.sum(wei, dim=-1, keepdim=True) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class FoolBabyGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFNkXJg8fi-y"
      },
      "outputs": [],
      "source": [
        "m = FoolBabyGPT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq8X27tOfi-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60532281-5ba2-46a3-9cd6-cc9afc1a8d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.0009: 100%|██████████| 1000/1000 [00:07<00:00, 133.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0008613121462985873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "m = m.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "# add tqdm progress bar with loss\n",
        "pbar = tqdm(range(1000))\n",
        "for steps in pbar:\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # update progress bar to display loss\n",
        "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oABOQXqfi-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e27e764-331f-4e2f-8791-01b0f7d44de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT, short for Generative Pre-trained Transformer, represents a groundbreaking advancement in the field of artificial intelligence and natural language processing. Deic oped by OpenAI, GPT is designed to understand, generate, and interpret r man language with remarkable accuracy and fluency. It operates on the principle of machine learning, where the model is initially pre-trained on a vast corpus of text data. This pre-training enables GPT to grasp the intricacies of language, including grammar, c\n"
          ]
        }
      ],
      "source": [
        "start_id = torch.tensor([encode('GPT')], dtype=torch.long)\n",
        "start_id = start_id.to(device)\n",
        "print(decode(m.generate(idx = start_id, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OBhpFwqsdVI"
      },
      "source": [
        "### Reference\n",
        "https://github.com/karpathy/nanoGPT"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}