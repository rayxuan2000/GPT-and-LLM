{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuneing LLAMA2 7B\n\nhttps://llama.meta.com/llama2/\n\nhttps://huggingface.co/blog/llama2\n\n\n![Alt text](https://th.bing.com/th/id/OIP.kqWhlzzjHwKJGfyAV1zNUgHaEK?w=307&h=180&c=7&r=0&o=5&dpr=2&pid=1.7 \"LLAMA2\")\n","metadata":{"id":"sl0QUnBLCGou"}},{"cell_type":"markdown","source":"\n\n---\n\n\n\n\n### LLM Training Process\n\n![Alt text](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lO2BB3K21OVvEuQC.png \"Training\")\n\n1. Pretraining\n2. Fine-tuning\n\n    2.2. SFT: Supervised Fine Tuning\n\n    2.3. RLHF: Reinforcement Learning from Human Feedback\n","metadata":{"id":"M8C3lugKdQsq"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"6Pi2Q6lOyttH","execution":{"iopub.status.busy":"2024-07-03T22:21:47.751559Z","iopub.execute_input":"2024-07-03T22:21:47.751967Z","iopub.status.idle":"2024-07-03T22:21:48.882636Z","shell.execute_reply.started":"2024-07-03T22:21:47.751934Z","shell.execute_reply":"2024-07-03T22:21:48.881325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git\n!pip install git+https://github.com/huggingface/peft.git\n!pip install git+https://github.com/huggingface/accelerate.git\n!pip install trl xformers wandb datasets einops gradio sentencepiece bitsandbytes huggingface_hub tensorboard","metadata":{"id":"Yv9j-zaWB7jd","execution":{"iopub.status.busy":"2024-07-03T22:21:48.885177Z","iopub.execute_input":"2024-07-03T22:21:48.885577Z","iopub.status.idle":"2024-07-03T22:23:30.235193Z","shell.execute_reply.started":"2024-07-03T22:21:48.885541Z","shell.execute_reply":"2024-07-03T22:23:30.233996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Libraries\n* transformers: This library provides APIs for downloading pre-trained models.\n* bitsandbytes: It’s a library for quantizing a large language model to reduce the memory footprint of the model, especially on GPUs.\n* peft: This is used to add a LoRA adapter to the LLM.\n* trl: This library contains an SFT (Supervised Fine-Tuning) class to fine-tune a model.\n* accelerate and xformers: These libraries are used to increase the inference speed of the model.\n* wandb/tensorboard: It’s used for monitoring the training process.\n* datasets: This library is used to load datasets from Hugging Face.\n* gradio: It’s used for designing simple user interfaces.","metadata":{"id":"en7dBxvTKELk"}},{"cell_type":"code","source":"import os,torch, wandb, platform, gradio, warnings\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n    TextStreamer,\n)\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\n","metadata":{"id":"C0VWJ7NjCZ6y","execution":{"iopub.status.busy":"2024-07-03T22:23:30.236693Z","iopub.execute_input":"2024-07-03T22:23:30.237024Z","iopub.status.idle":"2024-07-03T22:23:30.243285Z","shell.execute_reply.started":"2024-07-03T22:23:30.236993Z","shell.execute_reply":"2024-07-03T22:23:30.242372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### Prerequisites\n\nTo load our desired model, `meta-llama/Llama-2-7b-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n\n1. Gain access to the model on Hugging Face: [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n2. Use the Hugging Face CLI to login and verify your authentication status.\n\n\n![Alt Text](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5aZDn1DACKjcANXu.gif \"llm\")","metadata":{"id":"tmDkq13pZQhA"}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"id":"MVsEzMTACod2","execution":{"iopub.status.busy":"2024-07-03T22:23:30.245364Z","iopub.execute_input":"2024-07-03T22:23:30.245638Z","iopub.status.idle":"2024-07-03T22:23:30.276483Z","shell.execute_reply.started":"2024-07-03T22:23:30.245614Z","shell.execute_reply":"2024-07-03T22:23:30.275583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !huggingface-cli login","metadata":{"id":"C7p9BgVCY-DP","execution":{"iopub.status.busy":"2024-07-03T22:23:30.278009Z","iopub.execute_input":"2024-07-03T22:23:30.278431Z","iopub.status.idle":"2024-07-03T22:23:30.284064Z","shell.execute_reply.started":"2024-07-03T22:23:30.278400Z","shell.execute_reply":"2024-07-03T22:23:30.283201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_system_specs():\n    # Check if CUDA is available\n    is_cuda_available = torch.cuda.is_available()\n    print(\"CUDA Available:\", is_cuda_available)\n    # Get the number of available CUDA devices\n    num_cuda_devices = torch.cuda.device_count()\n    print(\"Number of CUDA devices:\", num_cuda_devices)\n    if is_cuda_available:\n        for i in range(num_cuda_devices):\n            # Get CUDA device properties\n            device = torch.device('cuda', i)\n            print(f\"--- CUDA Device {i} ---\")\n            print(\"Name:\", torch.cuda.get_device_name(i))\n            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n    # Get CPU information\n    print(\"--- CPU Information ---\")\n    print(\"Processor:\", platform.processor())\n    print(\"System:\", platform.system(), platform.release())\n    print(\"Python Version:\", platform.python_version())\nprint_system_specs()","metadata":{"id":"itOmJwuxUpvl","execution":{"iopub.status.busy":"2024-07-03T22:23:30.285185Z","iopub.execute_input":"2024-07-03T22:23:30.285465Z","iopub.status.idle":"2024-07-03T22:23:30.296827Z","shell.execute_reply.started":"2024-07-03T22:23:30.285436Z","shell.execute_reply":"2024-07-03T22:23:30.295964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### Loading Dataset, Model & Tokenizer\n\nHere, we are preparing our session by loading dataset and both the Llama2 model and its associated tokenizer.\n\nThe tokenizer will help in converting our text prompts into a format that the model can understand and process.","metadata":{"id":"lVjTj9p1Z3_S"}},{"cell_type":"code","source":"# Pre trained model\nbase_model = \"meta-llama/Llama-2-7b-hf\"\n\n# New instruction dataset\ndataset_name = \"vicgalle/alpaca-gpt4\"\n\n# New instruction dataset\n# guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n\n# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\nnew_model = \"https://huggingface.co/Rlele1002/Llama-2-7b-hf-ft\"","metadata":{"id":"TKrBO8ExEScb","execution":{"iopub.status.busy":"2024-07-03T22:23:30.298230Z","iopub.execute_input":"2024-07-03T22:23:30.298526Z","iopub.status.idle":"2024-07-03T22:23:30.306377Z","shell.execute_reply.started":"2024-07-03T22:23:30.298478Z","shell.execute_reply":"2024-07-03T22:23:30.305480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train[0:1000]\")\ndataset[\"text\"][0]","metadata":{"id":"rUP350jDEJ_B","execution":{"iopub.status.busy":"2024-07-03T22:23:30.307579Z","iopub.execute_input":"2024-07-03T22:23:30.307931Z","iopub.status.idle":"2024-07-03T22:23:31.709214Z","shell.execute_reply.started":"2024-07-03T22:23:30.307907Z","shell.execute_reply":"2024-07-03T22:23:31.708305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model(llama-2-7b-hf) and tokenizer\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True) #, padding_size=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n# tokenizer.padding_side = \"right\"","metadata":{"id":"h7RDYpSaEUzK","execution":{"iopub.status.busy":"2024-07-03T22:23:31.710355Z","iopub.execute_input":"2024-07-03T22:23:31.710640Z","iopub.status.idle":"2024-07-03T22:28:56.878046Z","shell.execute_reply.started":"2024-07-03T22:23:31.710614Z","shell.execute_reply":"2024-07-03T22:28:56.877022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#monitering login\nwandb.login(key=\"a11d38c20254fe0a56d631d4f5e66416e89b12ce\")\nrun = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:28:56.881876Z","iopub.execute_input":"2024-07-03T22:28:56.882184Z","iopub.status.idle":"2024-07-03T22:29:16.320010Z","shell.execute_reply.started":"2024-07-03T22:28:56.882158Z","shell.execute_reply":"2024-07-03T22:29:16.318911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lora config\npeft_config = LoraConfig(\n    lora_alpha= 8,\n    lora_dropout= 0.1,\n    r= 4,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:29:16.321395Z","iopub.execute_input":"2024-07-03T22:29:16.321757Z","iopub.status.idle":"2024-07-03T22:29:16.328506Z","shell.execute_reply.started":"2024-07-03T22:29:16.321721Z","shell.execute_reply":"2024-07-03T22:29:16.327442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_arguments = TrainingArguments(\n    output_dir= \"./results\",\n    logging_dir=\"./logs\",\n    num_train_epochs= 1,\n    per_device_train_batch_size= 1,\n    gradient_accumulation_steps= 2,\n    optim = \"paged_adamw_8bit\",\n    save_steps= 1000,\n    logging_steps= 30,\n    learning_rate= 2e-4,\n    weight_decay= 0.001,\n    fp16= True,\n    bf16= False,\n    max_grad_norm= 0.3,\n    max_steps= -1,\n    warmup_ratio= 0.3,\n    group_by_length= True,\n    lr_scheduler_type= \"linear\",\n    report_to=\"wandb\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:29:16.330176Z","iopub.execute_input":"2024-07-03T22:29:16.330532Z","iopub.status.idle":"2024-07-03T22:29:16.373735Z","shell.execute_reply.started":"2024-07-03T22:29:16.330498Z","shell.execute_reply":"2024-07-03T22:29:16.372762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SFTT Trainer arguments\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n) ","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:29:16.375011Z","iopub.execute_input":"2024-07-03T22:29:16.375374Z","iopub.status.idle":"2024-07-03T22:29:17.226564Z","shell.execute_reply.started":"2024-07-03T22:29:16.375340Z","shell.execute_reply":"2024-07-03T22:29:17.225528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:29:17.227896Z","iopub.execute_input":"2024-07-03T22:29:17.228212Z","iopub.status.idle":"2024-07-03T23:37:10.996437Z","shell.execute_reply.started":"2024-07-03T22:29:17.228182Z","shell.execute_reply":"2024-07-03T23:37:10.995323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:37:10.997893Z","iopub.execute_input":"2024-07-03T23:37:10.998263Z","iopub.status.idle":"2024-07-03T23:37:15.754762Z","shell.execute_reply.started":"2024-07-03T23:37:10.998228Z","shell.execute_reply":"2024-07-03T23:37:15.753876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stream(user_prompt):\n    runtimeFlag = \"cuda:0\"\n    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n\n    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:37:15.755887Z","iopub.execute_input":"2024-07-03T23:37:15.756232Z","iopub.status.idle":"2024-07-03T23:37:15.763026Z","shell.execute_reply.started":"2024-07-03T23:37:15.756204Z","shell.execute_reply":"2024-07-03T23:37:15.762076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stream(\"what is newtons 2rd law and its formula\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:37:15.764276Z","iopub.execute_input":"2024-07-03T23:37:15.764564Z","iopub.status.idle":"2024-07-03T23:38:08.829372Z","shell.execute_reply.started":"2024-07-03T23:37:15.764539Z","shell.execute_reply":"2024-07-03T23:38:08.828533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear the memory footprint\ndel model, trainer\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:38:08.830768Z","iopub.execute_input":"2024-07-03T23:38:08.831404Z","iopub.status.idle":"2024-07-03T23:38:09.035983Z","shell.execute_reply.started":"2024-07-03T23:38:08.831368Z","shell.execute_reply":"2024-07-03T23:38:09.035274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b-hf\" ","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:38:09.037224Z","iopub.execute_input":"2024-07-03T23:38:09.037515Z","iopub.status.idle":"2024-07-03T23:38:09.041483Z","shell.execute_reply.started":"2024-07-03T23:38:09.037491Z","shell.execute_reply":"2024-07-03T23:38:09.040637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_name, low_cpu_mem_usage=True,\n    return_dict=True,torch_dtype=torch.float16,\n    device_map= {\"\": 0})\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:38:09.042781Z","iopub.execute_input":"2024-07-03T23:38:09.043124Z","iopub.status.idle":"2024-07-03T23:39:41.351859Z","shell.execute_reply.started":"2024-07-03T23:38:09.043092Z","shell.execute_reply":"2024-07-03T23:39:41.350217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model)\ntokenizer.push_to_hub(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T23:39:41.352803Z","iopub.status.idle":"2024-07-03T23:39:41.353347Z","shell.execute_reply.started":"2024-07-03T23:39:41.353052Z","shell.execute_reply":"2024-07-03T23:39:41.353070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n#### Visualize model architecture","metadata":{"id":"_O8SAZDcaDtE"}},{"cell_type":"code","source":"model","metadata":{"id":"ezX9VP7PreS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### Creating the Llama Pipeline or Using Steaming Function\n\nWe'll set up a pipeline (or use steaming function) for text generation.\n\nThis pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n\n*Note*: This cell takes 2-3 minutes to run\n\nhttps://huggingface.co/docs/transformers/main_classes/pipelines\n","metadata":{"id":"EMigi2qCaOQV"}},{"cell_type":"code","source":"from transformers import pipeline\n\nllama_pipeline = pipeline(\n    \"text-generation\",  # LLM task\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\ndef get_response(user_prompt: str) -> None:\n    \"\"\"\n    Generate a response from the Llama model.\n\n    Parameters:\n        prompt (str): The user's input/question for the model.\n\n    Returns:\n        None: Prints the model's response.\n    \"\"\"\n    sequences = llama_pipeline(\n        user_prompt,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=256,\n    )\n    print(sequences[0]['generated_text'])\n","metadata":{"id":"roi_i3SVaW4B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stream(user_prompt: str) -> None:\n    runtimeFlag = \"cuda:0\"\n    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n\n    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)","metadata":{"id":"nUiBWjJQXGiu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_response(\"what is newtons 2nd law\")","metadata":{"id":"H6rO3sXua23G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before finetune\nstream(\"what is newtons 2nd law\")","metadata":{"id":"YtRLHBewXIvZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n---\n\n\n### Intilize traning parameters and Start the training process\n","metadata":{"id":"LX363TKzcKyj"}},{"cell_type":"markdown","source":"#### Step-by-step Guide to Fine-Tuning LLAMA2\n\nQLoRA (Quantized Low-Rank Adaptation) is an extension of LoRA (Low-Rank Adapters) that uses quantization to improve parameter efficiency during fine-tuning.\n\n![Alt Text](https://miro.medium.com/v2/resize:fit:1200/format:webp/0*AZpOOX-cjO_J8u9M.gif \"lora\")\n\nQLoRA / LoRA are techniques of Parameter-Efficient Fine-tuning (PEFT).\n\n![Alt Text](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJtZupeQVgp3s5HOBymcQw.png \"peft\")\n\nhttps://zhuanlan.zhihu.com/p/666234324\n\nhttps://zhuanlan.zhihu.com/p/623543497\n","metadata":{"id":"qJ7LwaIkHoPt"}},{"cell_type":"code","source":"# monitoring login\n# https://wandb.ai/authorize\n# wandb.login(key=\"\")\n# un = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")","metadata":{"id":"MBKy1LkJe5qb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha= 16,\n    lora_dropout= 0.1,\n    r= 64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n)","metadata":{"id":"IpS2lVpSLCEB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir= \"./results\",\n    num_train_epochs= 1,\n    per_device_train_batch_size= 6,\n    gradient_accumulation_steps= 2,\n    optim = \"paged_adamw_8bit\",\n    save_steps= 50,  # adjust based on your data\n    logging_steps= 25,\n    learning_rate= 2e-4,\n    weight_decay= 0.001,\n    fp16= False,\n    bf16= False,\n    max_grad_norm= 0.3,\n    max_steps= -1,\n    warmup_ratio= 0.3,\n    group_by_length= True,\n    lr_scheduler_type= \"linear\",\n    report_to=\"tensorboard\", # \"wandb\"\n)\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"id":"RMXKbpwwLbm_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"id":"DKHhNF2XMXBb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.tokenizer.save_pretrained(new_model)\n\n# wandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"id":"nJepEfnRQhOl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### Visualize Training Loss with Tensorboard","metadata":{"id":"oUOCi1SAfsFz"}},{"cell_type":"code","source":"from tensorboard import notebook\nlog_dir = \"./results/runs\"\nnotebook.start(\"--logdir {} --port 4001\".format(log_dir))","metadata":{"id":"QEKikem1T2UL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after finetune\nstream(\"what is newtons 2nd law\")","metadata":{"id":"9hrH02SxQthU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear the memory footprint\ndel model, trainer\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"id":"ubq6xTeJQuA7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"WCOKqxmU26Gn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_base_model = AutoModelForCausalLM.from_pretrained(\n    base_model, low_cpu_mem_usage=True,\n    return_dict=True,torch_dtype=torch.float16,\n    device_map= {\"\": 0})\nmodel = PeftModel.from_pretrained(loaded_base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"mgLKRAZ1Qznq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# upload fine-tuned model to hugginface repo\nmodel.push_to_hub(\"llama2_7b_ft_v2\", use_temp_dir=True)\ntokenizer.push_to_hub(\"llama2_7b_ft_v2\", use_temp_dir=True)","metadata":{"id":"DlO0u_2SQ2JE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"SQjN3gw82J39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### LLAMA2 with Gradio UI","metadata":{"id":"qt7DhpIY3AKQ"}},{"cell_type":"markdown","source":"\n\n---\n\n\n\n### Loading Model & Tokenizer\n\nhttps://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n\nHere, we are preparing our session by loading both the Llama model and its associated tokenizer.\n\nThe tokenizer will help in converting our text prompts into a format that the model can understand and process.","metadata":{"id":"-NHZKxCS3HgK"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"meta-llama/Llama-2-7b-chat-hf\" # https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n\ntokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)","metadata":{"id":"Lu-GWDnF2J7K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_pipeline = pipeline(\n    \"text-generation\",  # LLM task\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"id":"52L1p5d8T2UL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The struct of LLAMA2 prompts:\n```\n<s>[INST] <<SYS>>\n{{ system_prompt }}\n<</SYS>>\n\n{{ user_message }} [/INST]\n```","metadata":{"id":"G2TAj3cS8H3p"}},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\nYou are a helpful bot. Your answers are clear and concise.\n<</SYS>>\n\n\"\"\"\n\n# Formatting function for message and history\ndef format_message(message: str, history: list, memory_limit: int = 3) -> str:\n    \"\"\"\n    Formats the message and history for the Llama model.\n\n    Parameters:\n        message (str): Current message to send.\n        history (list): Past conversation history.\n        memory_limit (int): Limit on how many past interactions to consider.\n\n    Returns:\n        str: Formatted message string\n    \"\"\"\n    # always keep len(history) <= memory_limit\n    if len(history) > memory_limit:\n        history = history[-memory_limit:]\n\n    if len(history) == 0:\n        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n\n    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n\n    # Handle conversation history\n    for user_msg, model_answer in history[1:]:\n        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n\n    # Handle the current message\n    formatted_message += f\"<s>[INST] {message} [/INST]\"\n\n    return formatted_message","metadata":{"id":"Yk_sugXK3WEl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a response from the Llama model\ndef get_llama_response(message: str, history: list) -> str:\n    \"\"\"\n    Generates a conversational response from the Llama model.\n\n    Parameters:\n        message (str): User's input message.\n        history (list): Past conversation history.\n\n    Returns:\n        str: Generated response from the Llama model.\n    \"\"\"\n    query = format_message(message, history)\n    response = \"\"\n\n    sequences = llama_pipeline(\n        query,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=1024,\n    )\n\n    generated_text = sequences[0]['generated_text']\n    response = generated_text[len(query):]  # Remove the prompt from the output\n\n    print(\"Chatbot:\", response.strip())\n    return response.strip()","metadata":{"id":"3jpIGwE-8cW3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(get_llama_response(\"why the sky is blue?\", []))","metadata":{"id":"mECHy0zHBlZj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\n\ngr.ChatInterface(get_llama_response).launch()\n","metadata":{"id":"rqXkFSyu8dpX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ref\n\n####fine-tuning a llama-2\nhttps://gathnex.medium.com/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-dd79a788ac16","metadata":{"id":"Ae3idNWUT7vW"}},{"cell_type":"markdown","source":"### Future Work\n\n1. Try different fine-tuning dataset\n2. Try different base model such as CodeLLAMA / Mistral-7B","metadata":{"id":"5Tqw7sItJck2"}},{"cell_type":"code","source":"","metadata":{"id":"dGQycuHs8bu4"},"execution_count":null,"outputs":[]}]}